{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "from adversarial_generators.fgsm import generate_adversarial_images\n",
    "from model.VGG19 import VGG19\n",
    "from preprocess.preprocess import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config/config.yml', 'r') as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "\n",
    "ROOT_DIRECTORY = os.path.dirname(os.path.abspath('__file__'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load weights to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (config[\"img_height\"], config[\"img_width\"], 3)\n",
    "model = VGG19(input_shape = input_shape, num_classes = config[\"num_classes\"])\n",
    "model.load_weights(config[\"path_to_weights\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\auste\\anaconda3\\envs\\urop\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr = config[\"learning_rate\"], decay = config[\"learning_rate\"] / (config[\"epochs\"]))\n",
    "loss = config[\"loss_function\"]\n",
    "metrics = config[\"metrics\"]\n",
    "model.compile(optimizer = optimizer, loss = loss, metrics = [metrics])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data for adversarial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded: 0\n",
      "loaded: 500\n",
      "loaded: 1000\n",
      "loaded: 1500\n",
      "loaded: 2000\n",
      "loaded: 2500\n",
      "loaded: 3000\n",
      "loaded: 3500\n",
      "loaded: 4000\n",
      "loaded: 4500\n",
      "loaded: 5000\n",
      "loaded: 5500\n",
      "loaded: 6000\n",
      "loaded: 6500\n",
      "loaded: 7000\n",
      "loaded: 7500\n",
      "loaded: 8000\n",
      "loaded: 8500\n",
      "loaded: 9000\n",
      "loaded: 9500\n",
      "loaded: 10000\n",
      "loaded: 10500\n",
      "loaded: 11000\n",
      "loaded: 11500\n",
      "loaded: 12000\n",
      "loaded: 12500\n",
      "loaded: 13000\n",
      "loaded: 13500\n",
      "loaded: 14000\n",
      "loaded: 14500\n",
      "loaded: 15000\n",
      "loaded: 15500\n",
      "loaded: 16000\n",
      "loaded: 16500\n",
      "loaded: 17000\n",
      "loaded: 17500\n",
      "loaded: 18000\n",
      "loaded: 18500\n",
      "loaded: 19000\n",
      "loaded: 19500\n",
      "loaded: 20000\n",
      "loaded: 20500\n",
      "loaded: 21000\n",
      "loaded: 21500\n",
      "loaded: 22000\n",
      "loaded: 22500\n",
      "loaded: 23000\n",
      "loaded: 23500\n",
      "loaded: 24000\n",
      "loaded: 24500\n",
      "loaded: 25000\n",
      "loaded: 25500\n",
      "loaded: 26000\n",
      "loaded: 26500\n",
      "loaded: 27000\n",
      "loaded: 27500\n",
      "loaded: 28000\n",
      "loaded: 28500\n",
      "loaded: 29000\n",
      "loaded: 29500\n",
      "loaded: 30000\n",
      "loaded: 30500\n",
      "loaded: 31000\n",
      "loaded: 31500\n",
      "loaded: 32000\n",
      "loaded: 32500\n",
      "loaded: 33000\n",
      "loaded: 33500\n",
      "loaded: 34000\n",
      "loaded: 34500\n",
      "loaded: 35000\n",
      "loaded: 35500\n",
      "loaded: 36000\n",
      "loaded: 36500\n",
      "loaded: 37000\n",
      "loaded: 37500\n",
      "loaded: 38000\n",
      "loaded: 38500\n",
      "loaded: 39000\n",
      "loaded: 0\n",
      "loaded: 500\n",
      "loaded: 1000\n",
      "loaded: 1500\n",
      "loaded: 2000\n",
      "loaded: 2500\n",
      "loaded: 3000\n",
      "loaded: 3500\n",
      "loaded: 4000\n",
      "loaded: 4500\n",
      "loaded: 5000\n",
      "loaded: 5500\n",
      "loaded: 6000\n",
      "loaded: 6500\n",
      "loaded: 7000\n",
      "loaded: 7500\n",
      "loaded: 8000\n",
      "loaded: 8500\n",
      "loaded: 9000\n",
      "loaded: 9500\n",
      "loaded: 10000\n",
      "loaded: 10500\n",
      "loaded: 11000\n",
      "loaded: 11500\n",
      "loaded: 12000\n",
      "loaded: 12500\n"
     ]
    }
   ],
   "source": [
    "path_to_set = os.path.join(ROOT_DIRECTORY, config[\"path_to_data\"])\n",
    "path_to_train_csv = os.path.join(ROOT_DIRECTORY, config[\"path_to_train_csv\"])\n",
    "path_to_test_csv = os.path.join(ROOT_DIRECTORY, config[\"path_to_test_csv\"])\n",
    "(X_train, y_train) = load_data(path_to_train_csv, path_to_set, config[\"img_width\"], config[\"img_height\"])\n",
    "(X_test, y_test) = load_data(path_to_test_csv, path_to_set, config[\"img_width\"], config[\"img_height\"])\n",
    "X_test, X_adversarial_train, y_test, y_adversarial_train = train_test_split(X_test, y_test, test_size = 0.5, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_adversarial_train = X_adversarial_train.astype(\"float32\") / 255.0\n",
    "X_test = X_test.astype(\"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encode Target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, config[\"num_classes\"])\n",
    "y_adversarial_train = tf.keras.utils.to_categorical(y_adversarial_train, config[\"num_classes\"])\n",
    "y_test = tf.keras.utils.to_categorical(y_test, config[\"num_classes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot some adversarial images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nepsilons = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10]\\nindex = 6122 # Change if you want to see other images \\nimages = X_test[index:index + 64]\\nlabels = y_test[index:index + 64]\\nf, axarr = plt.subplots(5,11, figsize = (30, 20))\\nfor i in range(5):\\n  axarr[i, 0].imshow(images[i])\\n  axarr[i, 0].set_xlabel(\"Original class: {}\".format(np.argmax(labels, axis = 1)[i]))\\nfor i, eps in enumerate(epsilons):\\n  adversarial_images = generate_adversarial_images(images, labels, eps, model).numpy()\\n  new_predictions = model.predict_on_batch(adversarial_images)\\n  new_predictions = np.argmax(new_predictions, axis = 1)\\n  for ax in range(5):\\n    axarr[ax, i + 1].imshow(adversarial_images[ax])\\n    axarr[ax, i + 1].set_xlabel(\"New class: {}\".format(new_predictions[ax]))\\n  f.axes[i + 1].set_title(\\'Eps: {}\\'.format(eps))\\nplt.show()\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "epsilons = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10]\n",
    "index = 6122 # Change if you want to see other images \n",
    "images = X_test[index:index + 64]\n",
    "labels = y_test[index:index + 64]\n",
    "f, axarr = plt.subplots(5,11, figsize = (30, 20))\n",
    "for i in range(5):\n",
    "  axarr[i, 0].imshow(images[i])\n",
    "  axarr[i, 0].set_xlabel(\"Original class: {}\".format(np.argmax(labels, axis = 1)[i]))\n",
    "for i, eps in enumerate(epsilons):\n",
    "  adversarial_images = generate_adversarial_images(images, labels, eps, model).numpy()\n",
    "  new_predictions = model.predict_on_batch(adversarial_images)\n",
    "  new_predictions = np.argmax(new_predictions, axis = 1)\n",
    "  for ax in range(5):\n",
    "    axarr[ax, i + 1].imshow(adversarial_images[ax])\n",
    "    axarr[ax, i + 1].set_xlabel(\"New class: {}\".format(new_predictions[ax]))\n",
    "  f.axes[i + 1].set_title('Eps: {}'.format(eps))\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicate model to Model A and Model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model A\n",
    "model_a = tf.keras.models.clone_model(model)\n",
    "model_a.load_weights(config[\"path_to_weights\"])\n",
    "model_a.compile(optimizer = optimizer, loss = loss, metrics = [metrics])\n",
    "\n",
    "# Model B\n",
    "model_b = tf.keras.models.clone_model(model)\n",
    "model_b.load_weights(config[\"path_to_weights\"])\n",
    "model_b.compile(optimizer = optimizer, loss = loss, metrics = [metrics])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy of both models on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model A\n",
      "198/198 [==============================] - 26s 125ms/step - loss: 0.2334 - accuracy: 0.9601\n",
      "Model B\n",
      "198/198 [==============================] - 26s 128ms/step - loss: 0.2334 - accuracy: 0.9601\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.23339110612869263, 0.9600949883460999]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Model A')\n",
    "model_a.evaluate(X_test, y_test)\n",
    "print('Model B')\n",
    "model_b.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4657\n"
     ]
    }
   ],
   "source": [
    "eps = 0.05\n",
    "adversarial_images = generate_adversarial_images(X_adversarial_train, y_adversarial_train, eps, model_a).numpy()\n",
    "new_predictions = model.predict_on_batch(adversarial_images)\n",
    "\n",
    "indexes_of_wrong_images = (np.argmax(new_predictions, axis = 1) != np.argmax(y_adversarial_train, axis = 1))\n",
    "\n",
    "# Get the images wrongly classified by model A\n",
    "wrong_classified_images = X_adversarial_train[indexes_of_wrong_images]\n",
    "\n",
    "# Convert list of prob to one hot encoding for traing model B\n",
    "new_predictions = new_predictions[indexes_of_wrong_images]\n",
    "new_predictions = tf.keras.utils.to_categorical(np.argmax(new_predictions, axis = 1))\n",
    "print(len(new_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Callback for Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle train data for adversarial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha ratio between train and adv train data\n",
    "alpha = 5\n",
    "X_train_new, _, y_train_new, _ = train_test_split(X_train, y_train, train_size = alpha * len(wrong_classified_images), random_state = 0)\n",
    "X_for_A, y_for_A = sklearn.utils.shuffle(\n",
    "  np.concatenate((X_train_new, wrong_classified_images)),\n",
    "  np.concatenate((y_train_new, new_predictions)),\n",
    "  random_state = 0)\n",
    "X_for_B, y_for_B = sklearn.utils.shuffle(\n",
    "  np.concatenate((X_train_new, wrong_classified_images)),\n",
    "  np.concatenate((y_train_new, y_adversarial_train[indexes_of_wrong_images])),\n",
    "  random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model A with incorrect labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "393/393 [==============================] - 907s 2s/step - loss: 0.9588 - accuracy: 0.8254 - val_loss: 0.8951 - val_accuracy: 0.8290\n",
      "Epoch 2/20\n",
      "393/393 [==============================] - 909s 2s/step - loss: 0.8268 - accuracy: 0.8333 - val_loss: 0.8462 - val_accuracy: 0.8329\n",
      "Epoch 3/20\n",
      "393/393 [==============================] - 969s 2s/step - loss: 0.7595 - accuracy: 0.8403 - val_loss: 0.7677 - val_accuracy: 0.8426\n",
      "Epoch 4/20\n",
      "393/393 [==============================] - 1020s 3s/step - loss: 0.7052 - accuracy: 0.8467 - val_loss: 0.8003 - val_accuracy: 0.8347\n",
      "Epoch 5/20\n",
      "393/393 [==============================] - 1024s 3s/step - loss: 0.6443 - accuracy: 0.8545 - val_loss: 0.7331 - val_accuracy: 0.8404\n",
      "Epoch 6/20\n",
      "393/393 [==============================] - 1066s 3s/step - loss: 0.6166 - accuracy: 0.8551 - val_loss: 1.4626 - val_accuracy: 0.6501\n",
      "Epoch 7/20\n",
      "393/393 [==============================] - 1085s 3s/step - loss: 0.5994 - accuracy: 0.8569 - val_loss: 0.7457 - val_accuracy: 0.8411\n",
      "Epoch 8/20\n",
      "393/393 [==============================] - 1044s 3s/step - loss: 0.4987 - accuracy: 0.8749 - val_loss: 0.7352 - val_accuracy: 0.8479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ff0f21efd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a.fit(\n",
    "  x = X_for_A,\n",
    "  y = y_for_A,\n",
    "  batch_size = config[\"batch_size\"],\n",
    "  verbose = 1,\n",
    "  validation_split = 0.1,\n",
    "  callbacks = [callback],\n",
    "  epochs = config[\"epochs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model A\n",
      "198/198 [==============================] - 22s 112ms/step - loss: 1.8161 - accuracy: 0.6293\n",
      "Model B\n",
      "198/198 [==============================] - 24s 123ms/step - loss: 0.2334 - accuracy: 0.9601\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.23339110612869263, 0.9600949883460999]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Model A')\n",
    "model_a.evaluate(X_test, y_test)\n",
    "print('Model B')\n",
    "model_b.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model B with correct labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "393/393 [==============================] - 1036s 3s/step - loss: 0.3321 - accuracy: 0.9243 - val_loss: 0.1722 - val_accuracy: 0.9488\n",
      "Epoch 2/20\n",
      "393/393 [==============================] - 1050s 3s/step - loss: 0.0512 - accuracy: 0.9891 - val_loss: 0.0298 - val_accuracy: 0.9936\n",
      "Epoch 3/20\n",
      "393/393 [==============================] - 1071s 3s/step - loss: 0.0400 - accuracy: 0.9912 - val_loss: 0.0222 - val_accuracy: 0.9939\n",
      "Epoch 4/20\n",
      "393/393 [==============================] - 958s 2s/step - loss: 0.0215 - accuracy: 0.9951 - val_loss: 0.1651 - val_accuracy: 0.9664\n",
      "Epoch 5/20\n",
      "393/393 [==============================] - 901s 2s/step - loss: 0.0265 - accuracy: 0.9943 - val_loss: 0.0397 - val_accuracy: 0.9903\n",
      "Epoch 6/20\n",
      "393/393 [==============================] - 794s 2s/step - loss: 0.0197 - accuracy: 0.9961 - val_loss: 0.0259 - val_accuracy: 0.9939\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ff13e56910>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_b.fit(\n",
    "  x = X_for_B,\n",
    "  y = y_for_B,\n",
    "  batch_size = config[\"batch_size\"],\n",
    "  verbose = 1,\n",
    "  validation_split = 0.1,\n",
    "  callbacks = [callback],\n",
    "  epochs = config[\"epochs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_weights_a = os.path.join(config[\"path_to_save_weights\"], '{}_weight_a.h5'.format(eps))\n",
    "path_to_weights_b = os.path.join(config[\"path_to_save_weights\"], '{}_weight_b.h5'.format(eps))\n",
    "model_a.save_weights(os.path.join(ROOT_DIRECTORY, path_to_weights_a))\n",
    "model_b.save_weights(os.path.join(ROOT_DIRECTORY, path_to_weights_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "56cc440d07d7621ce49a7c460588d364d8586766d6362c42fe0a56848fc50829"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('urop': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
