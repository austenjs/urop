{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "import cv2\n",
    "import foolbox as fb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "from adversarial_generators.fgsm import generate_adversarial_images\n",
    "from model.VGG19 import VGG19\n",
    "from preprocess.preprocess import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config/config.yml', 'r') as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "\n",
    "ROOT_DIRECTORY = os.path.dirname(os.path.abspath('__file__'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load weights to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (config[\"img_height\"], config[\"img_width\"], 3)\n",
    "model = VGG19(input_shape = input_shape, num_classes = config[\"num_classes\"])\n",
    "model.load_weights(config[\"path_to_weights\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr = config[\"learning_rate\"], decay = config[\"learning_rate\"] / (config[\"epochs\"]))\n",
    "loss = config[\"loss_function\"]\n",
    "metrics = config[\"metrics\"]\n",
    "model.compile(optimizer = optimizer, loss = loss, metrics = [metrics])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data for adversarial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_set = os.path.join(ROOT_DIRECTORY, config[\"path_to_data\"])\n",
    "path_to_train_csv = os.path.join(ROOT_DIRECTORY, config[\"path_to_train_csv\"])\n",
    "path_to_test_csv = os.path.join(ROOT_DIRECTORY, config[\"path_to_test_csv\"])\n",
    "(X_train, y_train) = load_data(path_to_train_csv, path_to_set, config[\"img_width\"], config[\"img_height\"])\n",
    "(X_test, y_test) = load_data(path_to_test_csv, path_to_set, config[\"img_width\"], config[\"img_height\"])\n",
    "X_test, X_adversarial_train, y_test, y_adversarial_train = train_test_split(X_test, y_test, test_size = 0.5, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_adversarial_train = X_adversarial_train.astype(\"float32\") / 255.0\n",
    "X_test = X_test.astype(\"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encode Target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, config[\"num_classes\"])\n",
    "y_test = tf.keras.utils.to_categorical(y_test, config[\"num_classes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot some adversarial images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "epsilons = [0.001, 0.01, 0.03, 0.1]\n",
    "index = 213 # Change if you want to see other images \n",
    "images = X_test[index:index + 64]\n",
    "labels = y_test[index:index + 64]\n",
    "f, axarr = plt.subplots(5, 5, figsize = (30, 20))\n",
    "for i in range(5):\n",
    "  axarr[i, 0].imshow(images[i])\n",
    "  axarr[i, 0].set_xlabel(\"Original class: {}\".format(np.argmax(labels, axis = 1)[i]))\n",
    "for i, eps in enumerate(epsilons):\n",
    "  adversarial_images = generate_adversarial_images(images, labels, eps, model).numpy()\n",
    "  new_predictions = model.predict_on_batch(adversarial_images)\n",
    "  new_predictions = np.argmax(new_predictions, axis = 1)\n",
    "  for ax in range(5):\n",
    "    axarr[ax, i + 1].imshow(adversarial_images[ax])\n",
    "    axarr[ax, i + 1].set_xlabel(\"New class: {}\".format(new_predictions[ax]))\n",
    "  f.axes[i + 1].set_title('Eps: {}'.format(eps))\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicate model to Model A and Model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model A\n",
    "model_a = tf.keras.models.clone_model(model)\n",
    "model_a.load_weights(config[\"path_to_weights\"])\n",
    "model_a.compile(optimizer = optimizer, loss = loss, metrics = [metrics])\n",
    "\n",
    "# Model B\n",
    "model_b = tf.keras.models.clone_model(model)\n",
    "model_b.load_weights(config[\"path_to_weights\"])\n",
    "model_b.compile(optimizer = optimizer, loss = loss, metrics = [metrics])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Adversarial Training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_adversarial_train = tf.convert_to_tensor(X_adversarial_train)\n",
    "y_adversarial_train = tf.convert_to_tensor(y_adversarial_train)\n",
    "attack = fb.attacks.LinfPGD(abs_stepsize = 0.0078, steps = 7)\n",
    "bounds = (0, 1)\n",
    "preprocessing = dict()\n",
    "fmodel = fb.TensorFlowModel(model, bounds=bounds, preprocessing=preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Adversarial Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.031\n",
    "_, adversarial_images, indexes_of_wrong_images = attack(fmodel, X_adversarial_train, y_adversarial_train, epsilons = eps)\n",
    "\n",
    "# Get the images wrongly classified by model A\n",
    "indexes_of_wrong_images = indexes_of_wrong_images.numpy()\n",
    "adversarial_images = adversarial_images.numpy()\n",
    "wrong_classified_images = adversarial_images[indexes_of_wrong_images]\n",
    "\n",
    "# Convert list of prob to one hot encoding for traing model B\n",
    "new_predictions = model.predict_on_batch(wrong_classified_images)\n",
    "new_predictions = tf.keras.utils.to_categorical(np.argmax(new_predictions, axis = 1))\n",
    "print(len(new_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Callback for Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle train data for adversarial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha ratio between train and adv train data\n",
    "from sklearn.utils import resample\n",
    "\n",
    "alpha = 3\n",
    "X_train_new, y_train_new = resample(X_train, y_train, n_samples = alpha * len(wrong_classified_images), random_state = 0)\n",
    "y_adversarial_train = tf.keras.utils.to_categorical(y_adversarial_train, config[\"num_classes\"])\n",
    "X_for_B, y_for_B = sklearn.utils.shuffle(\n",
    "  np.concatenate((X_train_new, wrong_classified_images)),\n",
    "  np.concatenate((y_train_new, y_adversarial_train[indexes_of_wrong_images])),\n",
    "  random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1\n",
    "for i in range(N):\n",
    "  model_a.fit(\n",
    "    x = wrong_classified_images,\n",
    "    y = new_predictions,\n",
    "    batch_size = config[\"batch_size\"],\n",
    "    verbose = 1,\n",
    "    validation_split = 0.1,\n",
    "    callbacks = [callback],\n",
    "    epochs = config[\"epochs\"])\n",
    "\n",
    "  model_a.fit(\n",
    "    x = X_train_new,\n",
    "    y = y_train_new,\n",
    "    batch_size = config[\"batch_size\"],\n",
    "    verbose = 1,\n",
    "    validation_split = 0.1,\n",
    "    callbacks = [callback],\n",
    "    epochs = config[\"epochs\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b.fit(\n",
    "  x = X_for_B,\n",
    "  y = y_for_B,\n",
    "  batch_size = config[\"batch_size\"],\n",
    "  verbose = 1,\n",
    "  validation_split = 0.1,\n",
    "  callbacks = [callback],\n",
    "  epochs = config[\"epochs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model A')\n",
    "model_a.evaluate(X_test, y_test)\n",
    "print('Model B')\n",
    "model_b.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_weights_a = os.path.join('..\\weights3\\\\', 'PGD1_alpha={}_N={}_weight_a.h5'.format(alpha, N))\n",
    "path_to_weights_b = os.path.join('..\\weights3\\\\', 'PGD1_alpha={}_N={}_weight_b.h5'.format(alpha, N))\n",
    "model_a.save_weights(os.path.join(ROOT_DIRECTORY, path_to_weights_a))\n",
    "model_b.save_weights(os.path.join(ROOT_DIRECTORY, path_to_weights_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "56cc440d07d7621ce49a7c460588d364d8586766d6362c42fe0a56848fc50829"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('urop')",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
